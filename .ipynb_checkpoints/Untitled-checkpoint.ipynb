{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3519529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from math import sqrt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9ba1e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (122 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.1/122.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (914 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.7/914.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lokesh/miniconda3/envs/lokesh/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.9 smart-open-6.3.0 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e0b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBED_SIZE refers to representation size of each word\n",
    "# HIDDEN_SIZE refers to size of query, key and value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c8ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nHEADS = 8\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d209df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0.dev20230510+cu118\n"
     ]
    }
   ],
   "source": [
    "print(t.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e8ad8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8026, 0.8250, 0.4240,  ..., 0.1833, 0.9176, 0.2783],\n",
       "        [0.6592, 0.1254, 0.0827,  ..., 0.8300, 0.7167, 0.4494],\n",
       "        [0.4486, 0.2490, 0.2301,  ..., 0.5284, 0.8057, 0.7191],\n",
       "        ...,\n",
       "        [0.9032, 0.9598, 0.5788,  ..., 0.3619, 0.3243, 0.9281],\n",
       "        [0.4655, 0.7555, 0.4076,  ..., 0.7516, 0.3761, 0.7108],\n",
       "        [0.8270, 0.4645, 0.5747,  ..., 0.8043, 0.3223, 0.7804]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = t.rand(7,512)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50871ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96b4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncAttention, self).__init__()\n",
    "        self.Wq = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wk = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wv = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wo = nn.Parameter(t.rand(nHEADS*HIDDEN_SIZE, EMBED_SIZE))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Q = X@self.Wq\n",
    "        K = X@self.Wk\n",
    "        V = X@self.Wv\n",
    "        Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "        Z = nn.Softmax(dim=2)(Z)\n",
    "        Z = t.einsum('ijj->ij',[Z])\n",
    "        Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "        Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "        Z = Z@self.Wo\n",
    "        Z = nn.LayerNorm(Z.shape)(Z+X)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d717235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(512,2048)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(2048,512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "            \n",
    "    def forward(self,X):\n",
    "        Z = self.linear1(X)\n",
    "        Z = self.relu1(Z)\n",
    "        Z = self.linear2(Z)\n",
    "        Z = self.relu2(Z)\n",
    "        Z = nn.LayerNorm(Z.shape)(Z+X)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7069541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = EncAttention()\n",
    "        self.feedforward = FeedForward()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Z = self.attention(X)\n",
    "        Z = self.feedforward(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54bc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8cec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masked Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c3f739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8026, 0.8250, 0.4240,  ..., 0.1833, 0.9176, 0.2783],\n",
       "        [0.6592, 0.1254, 0.0827,  ..., 0.8300, 0.7167, 0.4494],\n",
       "        [0.4486, 0.2490, 0.2301,  ..., 0.5284, 0.8057, 0.7191],\n",
       "        ...,\n",
       "        [0.9032, 0.9598, 0.5788,  ..., 0.3619, 0.3243, 0.9281],\n",
       "        [0.4655, 0.7555, 0.4076,  ..., 0.7516, 0.3761, 0.7108],\n",
       "        [0.8270, 0.4645, 0.5747,  ..., 0.8043, 0.3223, 0.7804]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d332778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c593a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c540ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAttention, self).__init__()\n",
    "        self.Wq = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wk = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wv = nn.Parameter(t.rand(nHEADS, EMBED_SIZE, HIDDEN_SIZE))\n",
    "        self.Wo = nn.Parameter(t.rand(nHEADS*HIDDEN_SIZE, EMBED_SIZE))\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        Q = X@self.Wq\n",
    "        K = X@self.Wk\n",
    "        V = X@self.Wv\n",
    "        Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "        r,c = t.triu_indices(Z.shape[1],Z.shape[1],1)\n",
    "        Z[:,r,c] = float('-inf')\n",
    "        Z = nn.Softmax(dim=2)(Z)\n",
    "        Z = t.einsum('ijj->ij',[Z])\n",
    "        Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "        Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "        Z = Z@self.Wo\n",
    "        Z = nn.LayerNorm(Z.shape)(Z+X)\n",
    "        return Z, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94edb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncDecAttention,self).__init__()\n",
    "        self.getFromEncOutput1 = nn.Sequential(\n",
    "            nn.Linear(512,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.getFromEncOutput2 = nn.Sequential(\n",
    "            nn.Linear(512,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,maskquery,enc_output):\n",
    "        Q = maskquery\n",
    "        K = self.getFromEncOutput1(enc_output)\n",
    "        V = self.getFromEncOutput2(enc_output)\n",
    "        Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "        r,c = t.triu_indices(Z.shape[1],Z.shape[1],1)\n",
    "        Z[:,r,c] = float('-inf')\n",
    "        Z = nn.Softmax(dim=2)(Z)\n",
    "        Z = t.einsum('ijj->ij',[Z])\n",
    "        Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "        Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "        Z = Z@self.Wo\n",
    "        Z = nn.LayerNorm(Z.shape)(Z+X)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13caaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.masked = MaskedAttention()\n",
    "        self.encdec = EncDecAttention()\n",
    "        self.feedforward = FeedForward()\n",
    "        \n",
    "    def forward(self,X,enc_output):\n",
    "        Z, Q = self.masked(X)\n",
    "        Z = self.encdec(Q,enc_output)\n",
    "        Z = self.feedforward(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7024f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderStack,self).__init__()\n",
    "        self.enc1 = Encoder()\n",
    "        self.enc2 = Encoder()\n",
    "        self.enc3 = Encoder()\n",
    "        self.enc4 = Encoder()\n",
    "        self.enc5 = Encoder()\n",
    "        self.enc6 = Encoder()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Z = self.enc1(X)\n",
    "        Z = self.enc2(Z)\n",
    "        Z = self.enc3(Z)\n",
    "        Z = self.enc4(Z)\n",
    "        Z = self.enc5(Z)\n",
    "        Z = self.enc6(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8481c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderStack,self).__init__()\n",
    "        self.dec1 = Decoder()\n",
    "        self.dec2 = Decoder()\n",
    "        self.dec3 = Decoder()\n",
    "        self.dec4 = Decoder()\n",
    "        self.dec5 = Decoder()\n",
    "        self.dec6 = Decoder()\n",
    "        \n",
    "    def forward(self,X, enc_output):\n",
    "        Z = self.dec1(X, enc_output)\n",
    "        Z = self.dec2(Z, enc_output)\n",
    "        Z = self.dec3(Z, enc_output)\n",
    "        Z = self.dec4(Z, enc_output)\n",
    "        Z = self.dec5(Z, enc_output)\n",
    "        Z = self.dec6(Z, enc_output)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe7e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e594159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbd0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
