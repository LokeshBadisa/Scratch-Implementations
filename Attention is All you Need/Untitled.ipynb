{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3519529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Imports\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NLP Imports\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Miscellaneous Imports\n",
    "from os.path import exists\n",
    "from math import sqrt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2fd37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nHEADS is number of heads in multi-attention\n",
    "# d_model refers to representation size of each word\n",
    "# HIDDEN_SIZE refers to size of query, key and value vectors.\n",
    "nHEADS = 8\n",
    "d_model = 512\n",
    "HIDDEN_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635b61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device\n",
    "device = t.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d508cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fd043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Generator Style Dataset to Map Style Dataset\n",
    "train = to_map_style_dataset(train)\n",
    "test = to_map_style_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1343c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed005157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in source and target language\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return tokenize(text, spacy_de)\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return tokenize(text, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb99aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizes from each sentence in dataset \n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7a5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds Vocabulary of source and target language\n",
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427445ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads Vocabulary\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        t.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = t.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53fad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8315\n",
      "6384\n"
     ]
    }
   ],
   "source": [
    "source_vocab,target_vocab = load_vocab(spacy_de, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f78d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for length of target vocabulary\n",
    "#len(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35644ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for indices of each word\n",
    "#target_vocab(tokenize_en(\"Two young, White males are outside near many bushes.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a06a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this for vocabulary in target language\n",
    "#target_vocab.lookup_tokens(range(target_vocab.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ef76bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence pre-processing Helper Function\n",
    "def collate_batch(\n",
    "    batch,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    bs_id = t.tensor([0], device=device)  # <s> token id\n",
    "    eos_id = t.tensor([1], device=device)  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = t.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                t.tensor(\n",
    "                    source_vocab(tokenize_de(_src)),\n",
    "                    dtype=t.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ]\n",
    "        )\n",
    "        processed_tgt = t.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                t.tensor(\n",
    "                    target_vocab(tokenize_en(_tgt)),\n",
    "                    dtype=t.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ]\n",
    "        )\n",
    "        src_list.append(\n",
    "            # warning - overwrites values for negative values of padding - len\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src),\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = t.stack(src_list)\n",
    "    tgt = t.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5edf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate Function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    return collate_batch(\n",
    "    batch,\n",
    "    device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46592a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, data):\n",
    "        ans = t.empty_like(data)\n",
    "        for i,X in enumerate(data):\n",
    "            y = t.arange(self.d_model/2,device=device)        \n",
    "            y = t.repeat_interleave(y,2)\n",
    "            Z = t.empty((128,512),device=device)\n",
    "            for _ in range(Z.shape[0]):\n",
    "                Z[_] = y\n",
    "            Z = Z/self.d_model\n",
    "            Z = 1/(1e4)**Z\n",
    "            Z = t.arange(X.shape[1],device=device)*Z\n",
    "            Z[:, 0::2] = t.sin(Z[:, 0::2])\n",
    "            Z[:, 1::2] = t.cos(Z[:, 1::2])\n",
    "            Z = nn.Dropout(p=0.1)(Z)\n",
    "            ans[i] = nn.LayerNorm(Z.shape,device=device)(Z+X)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ac271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model,device=device)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.lut(X) * sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96b4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention in Encoder\n",
    "class EncAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncAttention, self).__init__()\n",
    "        self.Wq = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wk = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wv = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wo = nn.Parameter(t.rand(nHEADS*HIDDEN_SIZE, d_model)).to(device)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        ans = t.empty_like(data)\n",
    "        for i,X in enumerate(data):\n",
    "            X = X.to(device)\n",
    "            Q = X@self.Wq\n",
    "            K = X@self.Wk\n",
    "            V = X@self.Wv\n",
    "            Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "            Z = nn.Softmax(dim=2)(Z)\n",
    "            Z = t.einsum('ijj->ij',[Z])\n",
    "            Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "            Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "            Z = Z@self.Wo\n",
    "            Z = nn.Dropout(p=0.1)(Z)\n",
    "            Z = nn.LayerNorm(Z.shape,device=device)(Z+X)\n",
    "            ans[i] = Z\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d717235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(512,2048,device=device)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(2048,512,device=device)\n",
    "        self.relu2 = nn.ReLU()\n",
    "            \n",
    "    def forward(self,data):\n",
    "        ans = t.empty_like(data)\n",
    "        for i,X in enumerate(data):\n",
    "            Z = self.linear1(X)\n",
    "            Z = self.relu1(Z)\n",
    "            Z = self.linear2(Z)\n",
    "            Z = self.relu2(Z)\n",
    "            Z = nn.Dropout(p=0.1)(Z)\n",
    "            Z = nn.LayerNorm(Z.shape,device=device)(Z+X)\n",
    "            ans[i] = Z\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7069541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = EncAttention()\n",
    "        self.feedforward = FeedForward()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Z = self.attention(X)\n",
    "        Z = self.feedforward(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c540ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Attention in Decoder\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAttention, self).__init__()\n",
    "        self.Wq = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wk = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wv = nn.Parameter(t.rand(nHEADS, d_model, HIDDEN_SIZE)).to(device)\n",
    "        self.Wo = nn.Parameter(t.rand(nHEADS*HIDDEN_SIZE, d_model)).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self,data):\n",
    "        ans = t.empty_like(data)\n",
    "        for i,X in enumerate(data):\n",
    "            Q = X@self.Wq\n",
    "            K = X@self.Wk\n",
    "            V = X@self.Wv\n",
    "            Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "            r,c = t.triu_indices(Z.shape[1],Z.shape[1],1)\n",
    "            Z[:,r,c] = float('-inf')\n",
    "            Z = nn.Softmax(dim=2)(Z)\n",
    "            Z = t.einsum('ijj->ij',[Z])\n",
    "            Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "            Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "            Z = Z@self.Wo\n",
    "            Z = nn.Dropout(p=0.1)(Z)\n",
    "            Z = nn.LayerNorm(Z.shape,device=device)(Z+X)\n",
    "            ans[i] = Z\n",
    "        return ans.reshape(ans.shape[0],nHEADS,ans.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7748ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder Attention\n",
    "class EncDecAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncDecAttention,self).__init__()\n",
    "        self.Wo = nn.Parameter(t.rand(nHEADS*HIDDEN_SIZE, d_model)).to(device)\n",
    "        \n",
    "    def forward(self, maskquery, enc_output):\n",
    "        ans = t.empty_like(enc_output)\n",
    "        enc_output = enc_output.reshape(enc_output.shape[0],nHEADS,enc_output.shape[1],-1)\n",
    "        for i,X in enumerate(maskquery):\n",
    "            Q = X\n",
    "            K = enc_output[i]\n",
    "            V = enc_output[i]\n",
    "            Z = t.bmm(Q,K.transpose(1,2))/sqrt(HIDDEN_SIZE)\n",
    "            r,c = t.triu_indices(Z.shape[0],Z.shape[1],1)\n",
    "            Z[:,r,c] = float('-inf') \n",
    "            Z = nn.Softmax(dim=2)(Z)\n",
    "            Z = t.einsum('ijj->ij',[Z])           \n",
    "            Z = t.einsum('ij,ijk->ijk',Z,V)\n",
    "            Z = t.reshape(Z,(Z.shape[1],-1))\n",
    "            Z = Z@self.Wo\n",
    "            Z = nn.Dropout(p=0.1)(Z)           \n",
    "            Z = nn.LayerNorm(Z.shape,device=device)(Z+X.reshape(X.shape[1],-1))\n",
    "            ans[i] = Z            \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13caaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.masked = MaskedAttention()\n",
    "        self.encdec = EncDecAttention()\n",
    "        self.feedforward = FeedForward()\n",
    "        \n",
    "    def forward(self, X, enc_output):\n",
    "        Z = self.masked(X)\n",
    "        Z = self.encdec(Z, enc_output)\n",
    "        Z = self.feedforward(Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7024f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Stack\n",
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderStack,self).__init__()\n",
    "        self.enc1 = Encoder()\n",
    "        self.enc2 = Encoder()\n",
    "        self.enc3 = Encoder()\n",
    "        self.enc4 = Encoder()\n",
    "        self.enc5 = Encoder()\n",
    "        self.enc6 = Encoder()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        Z = self.enc1(X)\n",
    "#         print(\"Encoder 1 Completed\")\n",
    "        Z = self.enc2(Z)\n",
    "#         print(\"Encoder 2 Completed\")\n",
    "        Z = self.enc3(Z)\n",
    "#         print(\"Encoder 3 Completed\")\n",
    "        Z = self.enc4(Z)\n",
    "#         print(\"Encoder 4 Completed\")\n",
    "        Z = self.enc5(Z)\n",
    "#         print(\"Encoder 5 Completed\")\n",
    "        Z = self.enc6(Z)\n",
    "#         print(\"Encoder 6 Completed\")\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8481c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Stack\n",
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderStack,self).__init__()\n",
    "        self.dec1 = Decoder()\n",
    "        self.dec2 = Decoder()\n",
    "        self.dec3 = Decoder()\n",
    "        self.dec4 = Decoder()\n",
    "        self.dec5 = Decoder()\n",
    "        self.dec6 = Decoder()\n",
    "        \n",
    "    def forward(self,X, enc_output):\n",
    "        Z = self.dec1(X, enc_output)\n",
    "#         print(\"Decoder 1 Completed\")\n",
    "        Z = self.dec2(Z, enc_output)\n",
    "#         print(\"Decoder 2 Completed\")\n",
    "        Z = self.dec3(Z, enc_output)\n",
    "#         print(\"Decoder 3 Completed\")\n",
    "        Z = self.dec4(Z, enc_output)\n",
    "#         print(\"Decoder 4 Completed\")\n",
    "        Z = self.dec5(Z, enc_output)\n",
    "#         print(\"Decoder 5 Completed\")\n",
    "        Z = self.dec6(Z, enc_output)\n",
    "#         print(\"Decoder 6 Completed\")\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cafe7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Component of Fully-Connected and Softmax Layer\n",
    "class finalComponent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(finalComponent,self).__init__()\n",
    "        self.fc = nn.Linear(d_model,len(target_vocab)).to(device)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        ans = t.empty(data.shape[0],128,len(target_vocab))\n",
    "        for i,X in enumerate(data):\n",
    "            Z = self.fc(X)\n",
    "            Z = self.softmax(Z)\n",
    "            ans[i] = Z\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e618ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all components into 1 class\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.deu_embed = Embeddings(512,len(source_vocab))\n",
    "        self.eng_embed = Embeddings(512,len(target_vocab))\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.EncStack = EncoderStack()\n",
    "        self.DecStack = DecoderStack()\n",
    "        self.finalComponent = finalComponent()\n",
    "    \n",
    "    def forward(self, Xdeu,mode=\"train\"):\n",
    "        Xeng = t.full(Xdeu.shape,1,device=device)\n",
    "        Xeng[:,0] = 0\n",
    "        Xdeu = self.deu_embed(Xdeu)\n",
    "        Xdeu = self.pe(Xdeu)        \n",
    "        Z = self.EncStack(Xdeu)\n",
    "        Xeng = self.eng_embed(Xeng)              \n",
    "        Z = self.DecStack(Xeng,Z)\n",
    "        Z = self.finalComponent(Z)\n",
    "        if mode==\"train\":\n",
    "            return Z\n",
    "        else:\n",
    "            return t.argmax(Z,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a6a4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6df6c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fde9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db807954",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "warmup_steps = 4000\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = Adam(model.parameters(),betas=(0.9,0.98))\n",
    "lambda1 = lambda step_num : ((d_model)**(-0.5))*min((step_num+1)**(-0.5),(step_num+1)*(warmup_steps)**(-1.5))\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer,lr_lambda=lambda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daf5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c468f234ac324a17a2fec41216965b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097e637871bc4b27a434bb4165ec023b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "model.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for data in tqdm(train_dataloader):\n",
    "        model.zero_grad()\n",
    "        data = t.stack(list(data))\n",
    "        output = model(data[0])\n",
    "        target = t.zeros_like(output)\n",
    "        for i,X in enumerate(data[1]):\n",
    "            target[i][t.arange(X.size(0)),X] = 1\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        losses.append(loss.mean().item())\n",
    "        scheduler.step()\n",
    "model = model.to(t.device('cpu'))\n",
    "t.save(model,'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(num):\n",
    "    real = collate_fn([test[0]])\n",
    "    real = t.stack(list(real))\n",
    "    output = model(real[0],\"test\")\n",
    "    for i in output.squeeze():\n",
    "        print(target_vocab.lookup_token(i),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90ffe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
